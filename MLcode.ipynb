{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0PZIf2VHRpe",
        "outputId": "33f205bf-fd8d-4e2d-821c-737298fbd35d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#datasets\n",
        "print(\"Loading datasets...\")\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "interactions = pd.read_csv('interactions.csv')\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(f\"Interactions shape: {interactions.shape}\")\n",
        "\n",
        "# Convertdates to datetime (DD-MM-YYYY format)\n",
        "train['service_date'] = pd.to_datetime(train['service_date'], format='%d-%m-%Y')\n",
        "test['service_date'] = pd.to_datetime(test['service_date'], format='%d-%m-%Y')\n",
        "interactions['service_date'] = pd.to_datetime(interactions['service_date'], format='%d-%m-%Y')\n",
        "interactions['interaction_date'] = pd.to_datetime(interactions['interaction_date'], format='%d-%m-%Y')\n",
        "\n",
        "def create_features(df, interactions_df, is_train=True):\n",
        "    \"\"\"\n",
        "    Create features for prediction 15 days before service date\n",
        "    \"\"\"\n",
        "    print(\"\\nCreating features...\")\n",
        "    features_list = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if idx % 1000 == 0:\n",
        "            print(f\"Processing row {idx}/{len(df)}\")\n",
        "\n",
        "        service_date = row['service_date']\n",
        "        origin_hub = row['origin_hub_id']\n",
        "        dest_hub = row['destination_hub_id']\n",
        "\n",
        "        # Calculate cutoff date (15 days before service)\n",
        "        cutoff_date = service_date - timedelta(days=15)\n",
        "\n",
        "        # Filter interactions for this specific service up to cutoff date\n",
        "        service_interactions = interactions_df[\n",
        "            (interactions_df['service_date'] == service_date) &\n",
        "            (interactions_df['origin_hub_id'] == origin_hub) &\n",
        "            (interactions_df['destination_hub_id'] == dest_hub) &\n",
        "            (interactions_df['interaction_date'] <= cutoff_date)\n",
        "        ].sort_values('interaction_date')\n",
        "\n",
        "        # Initialize feature dictionary\n",
        "        feat = {\n",
        "            'service_date': service_date,\n",
        "            'origin_hub_id': origin_hub,\n",
        "            'destination_hub_id': dest_hub\n",
        "        }\n",
        "\n",
        "        if not is_train:\n",
        "            feat['service_key'] = row['service_key']\n",
        "\n",
        "        # Basic features\n",
        "        if len(service_interactions) > 0:\n",
        "            latest = service_interactions.iloc[-1]\n",
        "\n",
        "            # Latest cumulative values at cutoff\n",
        "            feat['latest_commitments'] = latest['cumulative_commitments']\n",
        "            feat['latest_interest'] = latest['cumulative_interest_signals']\n",
        "            feat['days_before_at_cutoff'] = latest['days_before_service']\n",
        "\n",
        "            # Maximum values seen\n",
        "            feat['max_commitments'] = service_interactions['cumulative_commitments'].max()\n",
        "            feat['max_interest'] = service_interactions['cumulative_interest_signals'].max()\n",
        "\n",
        "            # Velocity features (last 7 days before cutoff)\n",
        "            recent_interactions = service_interactions[\n",
        "                service_interactions['interaction_date'] >= (cutoff_date - timedelta(days=7))\n",
        "            ]\n",
        "\n",
        "            if len(recent_interactions) > 1:\n",
        "                commit_change = (recent_interactions.iloc[-1]['cumulative_commitments'] -\n",
        "                                recent_interactions.iloc[0]['cumulative_commitments'])\n",
        "                interest_change = (recent_interactions.iloc[-1]['cumulative_interest_signals'] -\n",
        "                                  recent_interactions.iloc[0]['cumulative_interest_signals'])\n",
        "                feat['commitment_velocity_7d'] = commit_change / 7\n",
        "                feat['interest_velocity_7d'] = interest_change / 7\n",
        "            else:\n",
        "                feat['commitment_velocity_7d'] = 0\n",
        "                feat['interest_velocity_7d'] = 0\n",
        "\n",
        "            # Ratio features\n",
        "            feat['commitment_to_interest_ratio'] = (feat['latest_commitments'] /\n",
        "                                                    (feat['latest_interest'] + 1))\n",
        "\n",
        "            # Activity features\n",
        "            feat['num_interactions'] = len(service_interactions)\n",
        "            feat['days_with_activity'] = service_interactions['interaction_date'].nunique()\n",
        "\n",
        "            # Categorical features\n",
        "            feat['origin_region'] = latest['origin_region']\n",
        "            feat['destination_region'] = latest['destination_region']\n",
        "            feat['origin_hub_tier'] = latest['origin_hub_tier']\n",
        "            feat['destination_hub_tier'] = latest['destination_hub_tier']\n",
        "\n",
        "        else:\n",
        "            # No interaction data available\n",
        "            feat['latest_commitments'] = 0\n",
        "            feat['latest_interest'] = 0\n",
        "            feat['days_before_at_cutoff'] = 15\n",
        "            feat['max_commitments'] = 0\n",
        "            feat['max_interest'] = 0\n",
        "            feat['commitment_velocity_7d'] = 0\n",
        "            feat['interest_velocity_7d'] = 0\n",
        "            feat['commitment_to_interest_ratio'] = 0\n",
        "            feat['num_interactions'] = 0\n",
        "            feat['days_with_activity'] = 0\n",
        "            feat['origin_region'] = 'unknown'\n",
        "            feat['destination_region'] = 'unknown'\n",
        "            feat['origin_hub_tier'] = 'unknown'\n",
        "            feat['destination_hub_tier'] = 'unknown'\n",
        "\n",
        "        # Temporal features\n",
        "        feat['day_of_week'] = service_date.dayofweek\n",
        "        feat['month'] = service_date.month\n",
        "        feat['day_of_month'] = service_date.day\n",
        "        feat['is_weekend'] = 1 if service_date.dayofweek >= 5 else 0\n",
        "        feat['quarter'] = service_date.quarter\n",
        "\n",
        "        # Hub combination features\n",
        "        feat['route'] = f\"{origin_hub}_{dest_hub}\"\n",
        "\n",
        "        if is_train:\n",
        "            feat['target'] = row['final_service_units']\n",
        "\n",
        "        features_list.append(feat)\n",
        "\n",
        "    return pd.DataFrame(features_list)\n",
        "\n",
        "# Create features\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "train_features = create_features(train, interactions, is_train=True)\n",
        "test_features = create_features(test, interactions, is_train=False)\n",
        "\n",
        "print(\"\\nTrain features shape:\", train_features.shape)\n",
        "print(\"Test features shape:\", test_features.shape)\n",
        "\n",
        "# Encode categorical variables\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ENCODING CATEGORICAL FEATURES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "categorical_cols = ['origin_region', 'destination_region', 'origin_hub_tier',\n",
        "                    'destination_hub_tier', 'route', 'origin_hub_id', 'destination_hub_id']\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    # Fit on combined data to ensure consistent encoding\n",
        "    combined = pd.concat([train_features[col].astype(str),\n",
        "                         test_features[col].astype(str)])\n",
        "    le.fit(combined)\n",
        "    train_features[col + '_encoded'] = le.transform(train_features[col].astype(str))\n",
        "    test_features[col + '_encoded'] = le.transform(test_features[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
        "\n",
        "# Select features for modeling\n",
        "feature_cols = [\n",
        "    'latest_commitments', 'latest_interest', 'max_commitments', 'max_interest',\n",
        "    'commitment_velocity_7d', 'interest_velocity_7d', 'commitment_to_interest_ratio',\n",
        "    'num_interactions', 'days_with_activity', 'days_before_at_cutoff',\n",
        "    'day_of_week', 'month', 'day_of_month', 'is_weekend', 'quarter',\n",
        "    'origin_region_encoded', 'destination_region_encoded',\n",
        "    'origin_hub_tier_encoded', 'destination_hub_tier_encoded',\n",
        "    'route_encoded', 'origin_hub_id_encoded', 'destination_hub_id_encoded'\n",
        "]\n",
        "\n",
        "X_train_full = train_features[feature_cols]\n",
        "y_train_full = train_features['target']\n",
        "X_test = test_features[feature_cols]\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X_train_full.shape}\")\n",
        "print(f\"Target shape: {y_train_full.shape}\")\n",
        "\n",
        "# Split for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}\")\n",
        "\n",
        "# Train models\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "print(\"\\n1. Training Gradient Boosting Regressor...\")\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"2. Training Random Forest Regressor...\")\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"3. Training Ridge Regression...\")\n",
        "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VALIDATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "gb_val_pred = gb_model.predict(X_val)\n",
        "rf_val_pred = rf_model.predict(X_val)\n",
        "ridge_val_pred = ridge_model.predict(X_val)\n",
        "\n",
        "ensemble_val_pred = (0.5 * gb_val_pred + 0.3 * rf_val_pred + 0.2 * ridge_val_pred)\n",
        "\n",
        "print(\"\\nGradient Boosting:\")\n",
        "print(f\"  MAE: {mean_absolute_error(y_val, gb_val_pred):.4f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_val, gb_val_pred)):.4f}\")\n",
        "print(f\"  R2: {r2_score(y_val, gb_val_pred):.4f}\")\n",
        "\n",
        "print(\"\\nRandom Forest:\")\n",
        "print(f\"  MAE: {mean_absolute_error(y_val, rf_val_pred):.4f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_val, rf_val_pred)):.4f}\")\n",
        "print(f\"  R2: {r2_score(y_val, rf_val_pred):.4f}\")\n",
        "\n",
        "print(\"\\nRidge Regression:\")\n",
        "print(f\"  MAE: {mean_absolute_error(y_val, ridge_val_pred):.4f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_val, ridge_val_pred)):.4f}\")\n",
        "print(f\"  R2: {r2_score(y_val, ridge_val_pred):.4f}\")\n",
        "\n",
        "print(\"\\nEnsemble (50% GB + 30% RF + 20% Ridge):\")\n",
        "print(f\"  MAE: {mean_absolute_error(y_val, ensemble_val_pred):.4f}\")\n",
        "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_val, ensemble_val_pred)):.4f}\")\n",
        "print(f\"  R2: {r2_score(y_val, ensemble_val_pred):.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RETRAINING ON FULL DATASET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "gb_model.fit(X_train_full, y_train_full)\n",
        "rf_model.fit(X_train_full, y_train_full)\n",
        "ridge_model.fit(X_train_full, y_train_full)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GENERATING TEST PREDICTIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "gb_test_pred = gb_model.predict(X_test)\n",
        "rf_test_pred = rf_model.predict(X_test)\n",
        "ridge_test_pred = ridge_model.predict(X_test)\n",
        "\n",
        "\n",
        "ensemble_test_pred = (0.5 * gb_test_pred + 0.3 * rf_test_pred + 0.2 * ridge_test_pred)\n",
        "\n",
        "\n",
        "ensemble_test_pred = np.maximum(0, ensemble_test_pred)\n",
        "\n",
        "print(f\"\\nPrediction statistics:\")\n",
        "print(f\"  Min: {ensemble_test_pred.min():.2f}\")\n",
        "print(f\"  Max: {ensemble_test_pred.max():.2f}\")\n",
        "print(f\"  Mean: {ensemble_test_pred.mean():.2f}\")\n",
        "print(f\"  Median: {np.median(ensemble_test_pred):.2f}\")\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'service_key': test_features['service_key'],\n",
        "    'final_service_units': ensemble_test_pred\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUBMISSION FILE CREATED\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nFile: submission.csv\")\n",
        "print(f\"Rows: {len(submission)}\")\n",
        "print(\"\\nFirst 10 predictions:\")\n",
        "print(submission.head(10))\n",
        "print(\"\\nâœ“ Training complete! Ready for submission.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--1WvFI4Jb-s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
